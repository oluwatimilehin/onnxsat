{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "crsid: oea27\n",
    "\n",
    "[tk - I need to check for typos and update citations; maybe don't number citations?]\n",
    "\n",
    "[tk - number the sections?]\n",
    "\n",
    "[search for all the figures and make sure they're labelled correctly]\n",
    "\n",
    "[update image captions and alts.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retraining-free Methods for Optimizing Deep Neural Networks \n",
    "\n",
    "Many state-of-the-art deep neural networks (DNNs) are large models with up to billions of parameters, which must be optimized for efficient deployment on edge devices. Optimization techniques include quantization, using knowledge distillation to train a smaller model, pruning, and so on.  [tk - is this accurate?]. Often, these methods require a form of retraining or finetuning to retain the desired accuracy. In this project, I explore techniques for optimizing a DNN that do not require any  training or finetuning. The aim is to investigate the performance gains we can attain for a pre-trained model with training data available.\n",
    "\n",
    "To do this, I built ONNXSat, a tool that accepts a pre-trained ONNX model and optimizes it to reduce its size and inference latency. Using ONNXSat, I investigate retraining-free optimization techniques on convolutional neural networks (CNNs) by conducting experiments to answer three questions:\n",
    "\n",
    "1. What is the effect of operator fusion on a model's performance?  \n",
    "2. How much sparsity can we introduce into a model while keeping accuracy loss within a threshold?\n",
    "3. How does sparsity affect a model's performance? - [tk - I might need to update this]\n",
    "\n",
    "\n",
    "[Results show that...tk: fill out]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The cells in this report are not executable, as importing all the logic used in ONNXSat would have made this writeup large. You can verify the code by running `python3 -m bench.bench <experiment_no> --d [optional results directory]` from the root directory to execute any benchmark. Alternatively, you can also run `python3 main.py` to test the end-to-end flow, or `make test` to execute the conversion tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Equality Saturation for Operator Fusion\n",
    "Operator fusion is a common optimization technique used in ML compilers to improve model performance by reducing the number of computations during inference. In this experiment, I evaluate the impact of operator fusion on model performance by implementing operator fusion in ONNXSat using a technique known as _equality saturation_ [1].\n",
    "\n",
    "Equality saturation addresses the phase-ordering problem in traditional compilers, where the order of applying optimizations affects the performance of a program. Borrowing the classic example from Willsey et al. [2], say a compiler has a rewrite rule stating that expressions of the form $x \\times 2$ should be written to use the left shift operator: $x \\ll 1$. \n",
    "\n",
    "If it receives an input expression $(a \\times 2)/2$, it might rewrite the program to $(a \\ll 2)/2$, which prevents simplifying the expression to $a$. The challenge here is that term rewriting in traditional compilers is destructive. After applying a rewrite, the original form is lost. \n",
    "\n",
    "Equality saturation uses an *equivalence graph* (e-graph) data structure to address the term rewriting problem. This e-graph contains *equivalence nodes* (e-nodes), which represent expressions in the program. Equivalent e-nodes belong to the same *equivalence class* (e-class), and edges point from e-nodes to child e-classes.\n",
    "\n",
    "During an exploration phase, an equality saturation engine builds the e-graph by applying user-provided rewrite rules to create new e-nodes until the e-graph has reached *saturation*, where no more rules can be applied. The engine then extracts the best version of the program according to a user-provided cost model. \n",
    "\n",
    "Figure 1 illustrates the e-graph for the expression in the classic example. Boxes with dotted borders represent e-classes, while e-nodes have solid borders. As shown in the figure, the e-graph contains all equivalent forms of the program until all the rewrites have been applied. With the e-graph structure, the order in which rewrites are applied does not affect the quality of the program. \n",
    "\n",
    "Instead, the engine extracts the program with the minimum cost based on the costs of the different expressions. \n",
    "\n",
    "<figure style=\"text-align: center; margin: 5px\">\n",
    "  <img\n",
    "  src=\"report/egraph.png\"\n",
    "  alt=\"Image containing three figures, each showing the resulting e-graph from applying rewrites to the input program.\"\n",
    "  style=\"width: 50%; height: auto;\">\n",
    "  <figcaption> <b>Figure 1. </b> E-graph for optimizing (a * 2)/2. Boxes with dotted borders represent e-classes, which contain equivalent e-nodes represented with solid borders. </figcaption>\n",
    "</figure>\n",
    "\n",
    "Prior work by TENSAT [3] and MCTS [4] uses equality saturation in Rust to optimize DNN computation graphs by applying rewrites generated by TASO [5]. ONNXSat differs from these by incorporating fusions not expressed by either of them to the best of my knowledge, and does not rely on TASO for its rewrite rules. In addition, it extends these by also supporting pruning neural network weights.  \n",
    "\n",
    "\n",
    "### 1.1 Building ONNXSat\n",
    "Given a pre-trained model in ONNX format, ONNXSat applies equality saturation to perform operator fusion and generate an optimized version of the model. ONNXSat uses *Egglog* [6], a Python library, as its equality saturation engine. At a high level, ONNXSat:\n",
    "\n",
    "- Converts an ONNX model to the Egglog format.\n",
    "- Applies pre-defined rewrites rules on the model using Egglog and extracts the best expression.\n",
    "- Converts the returned expression back into ONNX format for running inference. \n",
    "- Uses the ONNX Runtime Engine for inference.\n",
    "\n",
    "This section describes the different components in more detail. \n",
    "\n",
    "\n",
    "#### 1.1.1. ONNX to Egglog Conversion\n",
    "ONNXSat currently supports models containing 48 ONNX operators defined in [operators.py](eggie/operators.py).  For each operator, I created an equivalent method representing an Egglog e-node (or expression) according to the operator specification from ONNX. \n",
    "\n",
    "For example, the `MatMul` operator has a name, accepts two tensor inputs, $a$ and $b$, and produces an output. It is defined in ONNXSat as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@method()\n",
    "@classmethod\n",
    "def MatMul(cls, name: TensorId, a: TensorId, b: TensorId, output: TensorId) -> TensorId: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operators have attributes. For example, the `Conv` operator has attributes defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAttrs(Expr):\n",
    "    @method()\n",
    "    def __init__(\n",
    "        self,\n",
    "        auto_pad: String,\n",
    "        group: i64,\n",
    "        dilations: Vec[i64],\n",
    "        kernel_shape: Vec[i64],\n",
    "        pads: Vec[i64],\n",
    "        strides: Vec[i64],\n",
    "    ): ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Conv` operator is then represented as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@method()\n",
    "@classmethod\n",
    "def Conv(cls, \n",
    "        name: TensorId, \n",
    "        attrs: ConvAttrs,\n",
    "        x: TensorId,\n",
    "        w: TensorId,\n",
    "        b: TensorId,\n",
    "        output: TensorId,\n",
    "    ) -> TensorId: ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The `Op` class in [operators.py](eggie/operators.py) contains the definition of all the supported operators. It inherits from the Egglog `Expr` class, which is the base class for representing e-nodes.\n",
    "\n",
    "Each operator receives and returns `TensorId` objects, also defined in [operators.py](eggie/operators.py), making it easier to interchange between operators when expressing rewrite rules. \n",
    "\n",
    "The full definition of [operators.py](eggie/operators.py) is omitted from this writeup for brevity, as it is nearly 500 lines long.  \n",
    "\n",
    "\n",
    "The `OnnxParser` class defined in [onnx_e/parser.py](onnx_e/parser.py) contains methods to convert each supported ONNX operator node to its Egglog representation. \n",
    "\n",
    "For example, I define the method to convert an ONNX node representing a `MatMul` operation into the Egglog representation presented above as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_matmul(self, node: NodeProto) -> TensorId:\n",
    "    return Op.MatMul(\n",
    "        self._to_tensor_id(node.name),\n",
    "        self._to_tensor_id(node.input[0]),\n",
    "        self._to_tensor_id(node.input[1]),\n",
    "        self._to_tensor_id(node.output[0]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `NodeProto` represents an ONNX operator and `_to_tensor_id()` either returns an existing `TensorId` object if the node has been processed before, or returns a new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_tensor_id(self, id: str) -> TensorId:\n",
    "    if id in self._output_per_node:\n",
    "        return self._output_per_node[id]\n",
    "\n",
    "    return TensorId(id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ONNXParser` defines similar methods to `_convert_matmul()` for the supported operators, with additional processing for operators with attributes. The full class definition can be found in [onnx_e/parser.py](onnx_e/parser.py), containing approximately 800 lines. \n",
    "\n",
    "\n",
    "\n",
    "The conversion from ONNX to Egglog format is handled in the `Converter` class defined in [converter.py](converter.py) as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eggie.operators import *\n",
    "from eggie.parser import EggParser\n",
    "from egglog import Expr\n",
    "from onnx.onnx_ml_pb2 import GraphProto, NodeProto\n",
    "from typing import List\n",
    "\n",
    "from onnx_e.parser import OnnxParser\n",
    "\n",
    "\n",
    "class Converter:\n",
    "    @classmethod\n",
    "    def to_egglog(cls, graph: GraphProto) -> Expr:\n",
    "        return OnnxParser(graph).parse()\n",
    "\n",
    "    @classmethod\n",
    "    def to_onnx(cls, egglog: Expr) -> List[NodeProto]:\n",
    "        parser = EggParser(egglog)\n",
    "        return parser.parse()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `GraphProto` represents an ONNX computation graph. \n",
    "\n",
    "\n",
    "For an ONNX graph with the operator sequence: Conv -> Relu -> MaxPool, the resulting Egglog expression from the conversion logic has the form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "Op.MaxPool(\n",
    "    MaxPoolName,\n",
    "    MaxPoolAttributes,\n",
    "    Op.Relu(\n",
    "        ReluName,\n",
    "        Op.Conv(\n",
    "            ConvName,\n",
    "            ConvAttributes,\n",
    "            ConvInput,\n",
    "            ConvWeight,\n",
    "            ConvBias,\n",
    "            ConvOut,\n",
    "        ),\n",
    "        ReluOut,\n",
    "    ),\n",
    "    MaxPoolOut,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Op.Relu` takes the result of `Op.Conv` as its input, and its output is an input to `Op.MaxPool`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Rewrite Rules\n",
    "\n",
    "ONNXSat contains the following operation fusion rules, which were chosen based on common patterns in the evaluation model:\n",
    "\n",
    "- Conv + BatchNorm ->  FusedConvBatchNorm\n",
    "- Conv + BatchNorm + Relu -> FusedConvBatchNorm\n",
    "- Conv + Relu -> FusedConvActivation\n",
    "- Conv + Clip -> FusedConvActivation\n",
    "- Gemm + Relu -> FusedGemm\n",
    "\n",
    "\n",
    "To add rules, we define a ruleset for Egglog and register rules to the ruleset. For example, the rule to rewrite a Gemm + Relu sequence to a FusedGemm operator is defined in [rewrites.py](eggie/rewrites.py) as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fusion_ruleset = ruleset(name=\"fusion_ruleset\")\n",
    "\n",
    "@fusion_ruleset.register\n",
    "def fuse_gemm_relu(\n",
    "    gemm_name: TensorId,\n",
    "    gemm_in: Vec[TensorId],\n",
    "    alpha: f64,\n",
    "    beta: f64,\n",
    "    transA: i64,\n",
    "    transB: i64,\n",
    "    gemm_out: TensorId,\n",
    "    relu_name: TensorId,\n",
    "    relu_out: TensorId,\n",
    "):\n",
    "    yield rewrite(\n",
    "        Op.Relu(\n",
    "            relu_name,\n",
    "            Op.Gemm(\n",
    "                gemm_name,\n",
    "                GemmAttrs(alpha, beta, transA, transB),\n",
    "                gemm_in,\n",
    "                gemm_out,\n",
    "            ),\n",
    "            relu_out,\n",
    "        )\n",
    "    ).to(\n",
    "        Op.FusedGemm(\n",
    "            gemm_name,\n",
    "            TensorId(\"com.microsoft\"),\n",
    "            FusedGemmAttrs(\n",
    "                \"Relu\", f64(0.0), f64(0.0), f64(0.0), alpha, beta, transA, transB\n",
    "            ),\n",
    "            gemm_in,\n",
    "            relu_out,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet instructs Egglog to rewrite an expression containing an `Op.Relu` e-node receiving the result of an `Op.Gemm` node to an `Op.FusedGemm` expression.\n",
    "\n",
    "Egglog applies all the rewrite rules that match expressions in a given computation graph until it reaches saturation and extracts the best program according to the cost model. \n",
    "\n",
    "The cost model for ONNXSat is simple: all the operators have the same Egglog default cost of 1. However, since the rewrite rules fuse multiple operators into one, Egglog will select the single fused operator as the best e-node from an e-class, as that will yield a lower cost expression than one with multiple operators. \n",
    "\n",
    "#### 1.1.3. Egglog to ONNX Conversion\n",
    "\n",
    "ONNXSat converts the extracted Egglog expression back into ONNX format using `Lexer` and `EggParser` classes defined in [lexer.py](eggie/lexer.py) and [parser.py](eggie/parser.py) respectively. The lexer splits the expression into tokens that the parser uses to create ONNX nodes. \n",
    "\n",
    "The `EgglogTokenKind` enum defined in [lexer.py](eggie/lexer.py) represents the expected input tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EgglogTokenKind(Enum):\n",
    "    AVG_POOL_ATTRS = auto()\n",
    "    BATCH_NORM_ATTRS = auto()\n",
    "    COMMA = auto()\n",
    "    CONV_ATTRS = auto()\n",
    "    DOT = auto()\n",
    "    EQUALS = auto()\n",
    "    EOF = auto()\n",
    "    FLOAT_LITERAL = auto()\n",
    "    FUSED_CONV_ATTRS = auto()\n",
    "    FUSED_GEMM_ATTRS = auto()\n",
    "    GEMM_ATTRS = auto()\n",
    "    LEFT_PARENTHESIS = auto()\n",
    "    LEFT_SQUARE_BRACKET = auto()\n",
    "    MAX_POOL_ATTRS = auto()\n",
    "    INTEGER_LITERAL = auto()\n",
    "    OP = auto()\n",
    "    QUANTIZE_LINEAR_ATTRS = auto()\n",
    "    RIGHT_PARENTHESIS = auto()\n",
    "    RIGHT_SQUARE_BRACKET = auto()\n",
    "    STRING_LITERAL = auto()\n",
    "    TENSOR_ID = auto()\n",
    "    TENSOR_TYPE = auto()\n",
    "    VARIABLE_NAME = auto()\n",
    "    VEC = auto()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the file contains a mapping from string values to the different tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_token = {\n",
    "    \"AveragePoolAttrs\": EgglogTokenKind.AVG_POOL_ATTRS,\n",
    "    \"BatchNormAttrs\": EgglogTokenKind.BATCH_NORM_ATTRS,\n",
    "    \"ConvAttrs\": EgglogTokenKind.CONV_ATTRS,\n",
    "    \"FusedConvAttrs\": EgglogTokenKind.FUSED_CONV_ATTRS,\n",
    "    \"FusedGemmAttrs\": EgglogTokenKind.FUSED_GEMM_ATTRS,\n",
    "    \"GemmAttrs\": EgglogTokenKind.GEMM_ATTRS,\n",
    "    \"MaxPoolAttrs\": EgglogTokenKind.MAX_POOL_ATTRS,\n",
    "    \"Op\": EgglogTokenKind.OP,\n",
    "    \"QuantizeLinearAttrs\": EgglogTokenKind.QUANTIZE_LINEAR_ATTRS,\n",
    "    \"TensorId\": EgglogTokenKind.TENSOR_ID,\n",
    "    \"TensorType\": EgglogTokenKind.TENSOR_TYPE,\n",
    "    \"Vec\": EgglogTokenKind.VEC,\n",
    "    \"=\": EgglogTokenKind.EQUALS,\n",
    "    \"[\": EgglogTokenKind.LEFT_SQUARE_BRACKET,\n",
    "    \"]\": EgglogTokenKind.RIGHT_SQUARE_BRACKET,\n",
    "    \"(\": EgglogTokenKind.LEFT_PARENTHESIS,\n",
    "    \")\": EgglogTokenKind.RIGHT_PARENTHESIS,\n",
    "    \",\": EgglogTokenKind.COMMA,\n",
    "    \".\": EgglogTokenKind.DOT,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Lexer` class has a `next_token()` method that provides the next unconsumed input token to the `EggParser` class. \n",
    "\n",
    "\n",
    "The `EggParser` parses tokens from the `Lexer` until there are no more tokens left to process. When it receives an `EgglogTokenKind.OP` token, it invokes the relevant method to parse an operator. \n",
    "\n",
    "For example, if the input contains `Op.MatMul`, it invokes the `_parse_matmul()` method to create a `MatMul` node. This method is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_matmul(self) -> NodeProto:\n",
    "    return self._get_node(\n",
    "        \"MatMul\",\n",
    "        input_types=[EgglogTokenKind.TENSOR_ID, EgglogTokenKind.TENSOR_ID],\n",
    "        output_types=[EgglogTokenKind.TENSOR_ID],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All operator parsing functions use the `_get_node()` helper method defined in the `Parser` class to create an ONNX node. This method has the signature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_node(\n",
    "    self,\n",
    "    op_type: str,\n",
    "    input_types: List[EgglogTokenKind],\n",
    "    output_types: List[EgglogTokenKind],\n",
    "    attributes: List[Attribute] = [],\n",
    "    has_domain: bool = False,\n",
    ") -> NodeProto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_get_node()` method parses the inputs, outputs, and any attributes for a given operator and creates an ONNX node. Additionally, if the operator is not a standard ONNX operator but has been contributed by another and so has a custom domain [tk - rewrite], the method will parse the domain. \n",
    "\n",
    "`EggParser` returns a list of all the ONNX nodes when there are no more inputs to consume. \n",
    "\n",
    "#### 1.1.4. Putting it all together\n",
    "\n",
    "The `ModelOptimizer` class defined in [model_optimizer.py](model_optimizer.py) connects the different pieces together in its `run()` method, which operates on a given computation graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(self, ...) -> ModelProto:\n",
    "    ....    \n",
    "    egg_expr = Converter.to_Egglog(graph)\n",
    "    egraph = EGraph(save_Egglog_string=True)\n",
    "    egg_expr = egraph.let(\"expr\", egg_expr)\n",
    "    ...\n",
    "    # Run equality saturation until the given limit or all the rules have been applied\n",
    "    egraph.run(limit=1000, ruleset=fusion_ruleset)\n",
    "    extracted = egraph.extract(egg_expr)\n",
    "    ...\n",
    "    converted_onnx_nodes = Converter.to_onnx(extracted)\n",
    "    new_model = self._update(new_model, converted_onnx_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After converting back to ONNX, there is an extra processing step for the `FusedConvBatchNorm` operator. Unlike the `FusedConvActivation` operator which uses a Microsoft-provided `FusedConv` [7] ONNX operator in the `com.microsoft` domain, there is no public ONNX operator that combines a convolution and batch normalization.\n",
    "\n",
    "I implement this fusion in the `_process_conv_bn` method of the `ModelOptimizer` class, creating new weights and bias tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process_conv_bn(self, node: gs.Node) -> gs.Node:\n",
    "    ... # Skip input parsing\n",
    "    \n",
    "    # Fold the batch normalization into the convolution and update the conv bias and weights \n",
    "    adjusted_scale = bn_scale / np.sqrt(bn_input_var + epsilon)\n",
    "\n",
    "    new_w = conv_w * adjusted_scale[:, None, None, None]\n",
    "    new_b = (conv_bias - bn_input_mean) * adjusted_scale + bn_bias\n",
    "\n",
    "    ... # Skip new node creation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hubens [8] explains this fusion in more detail, but in essence, because convolution and batch normalization are linear operations, they can be combined to create a single linear operation. [tk - anything else?]\n",
    "\n",
    "The result of the `ModelOptimizer.run()` function is an optimized ONNX model containing fused operators. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Evaluation\n",
    "\n",
    "To evaluate ONNXSat, I compared the inference latency for its generated model with the optimizations from the ONNX Runtime Engine against a baseline of no optimizations on five CNN models, four of which are standard models: MobileNetV2 [9], MobileNetV4 [10], ResNet18 [11], and ShuffleNetV2 [12] trained on the ImageNet 2012 dataset [13].\n",
    "\n",
    "The final model is a simple classifier from a previous lab trained on the CIFAR-10 [14] dataset and defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"Simple CNN adapted from 'PyTorch: A 60 Minute Blitz'.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2 below illustrates the result of optimizing the simple classifier using ONNXSat. The graph on the left is the original form and the one on the right is the optimized one obtained by fusing operators. \n",
    "\n",
    "<figure style=\"text-align: center; margin: 5px\">\n",
    "  <p float=\"left\"> \n",
    "  <img\n",
    "  src=\"report/simple_classifier_orig.png\"\n",
    "  style=\"height:20%;\">\n",
    "    <img\n",
    "  src=\"report/simple_classifier_fused.png\"\n",
    "  style=\"height:20%; margin-left: 60px; margin-bottom: 50px\">\n",
    "  <figcaption> <b>Figure 2. </b> Result of applying ONNXSat on the simple classifier model. The image on the left is the original model and the optimized ONNXSat model is on the right.  </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "All experiments were run on an Apple MacBook Air with an Apple M2 chip running macOS 15 with 16GB memory. The reported results use the median latency value obtained by measuring the latency over 3 runs, each performing inference on 2000 samples from the relevant dataset. The accuracy after each run is also verified to be unchanged. \n",
    "\n",
    "\n",
    "The `BenchmarkTool` class in [bench.py](bench/bench.py) contains methods for executing all the benchmarks. The code to execute this experiment is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named tuples that serve as types for the benchmarking\n",
    "Accuracy = namedtuple(\"Accuracy\", \"top_one top_five\")\n",
    "Latency = namedtuple(\"Latency\", \"mean median percentile_95\")\n",
    "Benchmark = namedtuple(\"Benchmark\", \"num_samples accuracy latency throughput\")\n",
    "\n",
    "\n",
    "class BenchMarkCategory(Enum):\n",
    "    BASELINE = auto()\n",
    "    ONNX_SAT = auto()\n",
    "    ONNX_OPT = auto()\n",
    "    ONNX_SAT_AND_OPT = auto()\n",
    "\n",
    "\n",
    "category_to_display_name = {\n",
    "    BenchMarkCategory.BASELINE: \"Baseline - No Optimization\",\n",
    "    BenchMarkCategory.ONNX_SAT: \"OnnxSAT\",\n",
    "    BenchMarkCategory.ONNX_OPT: \"Onnx Optimizations\",\n",
    "    BenchMarkCategory.ONNX_SAT_AND_OPT: \"OnnxSAT + OnnxOptimizations\",\n",
    "}\n",
    "\n",
    "def experiment_one(self) -> None:\n",
    "    results: Dict[str, Dict[BenchMarkCategory, Benchmark]] = {}\n",
    "    results_dir = f\"{self._results_dir}/1\"\n",
    "\n",
    "    if not os.path.exists(results_dir):\n",
    "        os.makedirs(results_dir)\n",
    "\n",
    "    for model_name, orig_model in self._model_name_to_file.items():        \n",
    "        # Apply optimizations in offline mode, so optimization overhead does not affect inference measurement \n",
    "        # https://onnxruntime.ai/docs/performance/model-optimizations/graph-optimizations.html#onlineoffline-mode\n",
    "        onnx_optimized_model = (\n",
    "            f\"{self._onnx_optimized_models_dir}/{model_name}.onnx\"\n",
    "        )\n",
    "\n",
    "        # This method enables all available ONNX optimizations and stores the resulting ONNX file in the file path above\n",
    "        self.apply_onnx_opt_offline(model_name, orig_model, onnx_optimized_model)\n",
    "\n",
    "        new_model = None\n",
    "        with open(orig_model, \"rb\"):\n",
    "            model_optimizer = ModelOptimizer(\n",
    "                orig_model, self._data_dir, results_dir\n",
    "            )\n",
    "            new_model = model_optimizer.run()\n",
    "\n",
    "        # Offline-mode optimizations to measure the inference latency see of an ONNXSat + ONNX Runtime Engine-optimized model\n",
    "        eqsat_and_onnx_optimized_model = (\n",
    "            f\"{self._onnx_optimized_models_dir}/{model_name}-eqsat.onnx\"\n",
    "        )\n",
    "        self.apply_onnx_opt_offline(\n",
    "            model_name, new_model, eqsat_and_onnx_optimized_model\n",
    "        )\n",
    "\n",
    "        category_to_model = {\n",
    "            BenchMarkCategory.ONNX_SAT: new_model,\n",
    "            BenchMarkCategory.ONNX_OPT: onnx_optimized_model,\n",
    "            BenchMarkCategory.ONNX_SAT_AND_OPT: eqsat_and_onnx_optimized_model,\n",
    "            BenchMarkCategory.BASELINE: orig_model,\n",
    "        }\n",
    "\n",
    "        category_results = {}\n",
    "        warmup_loader, num_labels = self._get_loader_and_num_labels(\n",
    "            model_name, num_samples=1000\n",
    "        )\n",
    "        loader, num_labels = self._get_loader_and_num_labels(\n",
    "            model_name, num_samples=2000\n",
    "        )\n",
    "\n",
    "        for category, model in category_to_model.items():\n",
    "            sess_options = ort.SessionOptions()\n",
    "            sess_options.graph_optimization_level = (\n",
    "                ort.GraphOptimizationLevel.ORT_DISABLE_ALL # Disable online-mode optimizations\n",
    "            )\n",
    "            session = ort.InferenceSession(model, sess_options=sess_options)\n",
    "\n",
    "            # Warm up\n",
    "            self._measure(session, warmup_loader, num_labels)\n",
    "\n",
    "            # Actual Measurment\n",
    "            category_results[category] = self._measure(\n",
    "                session, loader, num_labels, num_runs=3\n",
    "            )\n",
    "\n",
    "        results[model_name] = category_results\n",
    "\n",
    "    self._visualize_latency_and_throughput(1, category_to_model.keys(), results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `_measure()` method, which runs inference and reports the benchmark values for each model across all experiments is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _measure(\n",
    "    self,\n",
    "    session: ort.InferenceSession,\n",
    "    dataloader: DataLoader,\n",
    "    num_labels=1000,\n",
    "    num_runs=1,\n",
    ") -> Benchmark:\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    output_name = session.get_outputs()[0].name\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    latencies = []\n",
    "\n",
    "    overall_start_time = time.perf_counter()\n",
    "\n",
    "    for _ in range(num_runs):\n",
    "        run_latencies = []\n",
    "        for inputs, labels in tqdm(\n",
    "            dataloader, desc=\"Inference Progress\", unit=\"batch\"\n",
    "        ):\n",
    "            inputs_numpy = inputs.numpy()\n",
    "            labels_numpy = labels.numpy()\n",
    "\n",
    "        # Run inference\n",
    "        start_time = time.perf_counter()\n",
    "        outputs = session.run([output_name], {input_name: inputs_numpy})\n",
    "        end_time = time.perf_counter()\n",
    "        \n",
    "        run_latencies.append(end_time - start_time)\n",
    "\n",
    "        all_preds.append(outputs[0])\n",
    "        all_labels.append(labels_numpy)\n",
    "\n",
    "        latencies.extend(run_latencies)\n",
    "\n",
    "    overall_end_time = time.perf_counter()\n",
    "    throughput = len(latencies) / (overall_end_time - overall_start_time)\n",
    "\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "    top1_accuracy = top_k_accuracy_score(\n",
    "            all_labels, all_preds, k=1, labels=np.arange(num_labels)\n",
    "        )\n",
    "    top5_accuracy = top_k_accuracy_score(\n",
    "            all_labels, all_preds, k=5, labels=np.arange(num_labels)\n",
    "        )\n",
    "\n",
    " \n",
    "    accuracy = Accuracy(top1_accuracy, top5_accuracy)\n",
    "\n",
    "    mean_latency = np.mean(latencies)\n",
    "    median = np.median(latencies)\n",
    "    percentile_95 = np.percentile(latencies, 95)\n",
    "\n",
    "    latency = Latency(mean_latency, median, percentile_95)\n",
    "\n",
    "    return Benchmark(len(latencies), accuracy, latency, throughput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### 1.2.1. Results\n",
    "\n",
    "Figure 3 shows the median latency speedup for different optimization methods against a baseline of no optimizations. The optimization passes obtain a speedup of 1.03x to 1.24x for all models over the baseline. \n",
    "\n",
    "\n",
    "<figure style=\"text-align: center; margin: 5px\">\n",
    "  <img\n",
    "  src=\"report/1_latency.png\"\n",
    "  alt=\"Image containing five grouped bars demonstrating the speedup of different optimizations against a baseline with no optimizations\"\n",
    "  style=\"width: 50%; height: auto;\">\n",
    "  <figcaption> <b>Figure 3. </b> Median latency speedup against a baseline with no optimizations for five CNN models. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Figure 4 displays the throughput of each optimization method against the baseline. The throughput does not always correlate with the latency values, but we still observe throughput gains from operator fusion for the state-of-the-art models. \n",
    "\n",
    "<figure style=\"text-align: center; margin: 5px\">\n",
    "  <img\n",
    "  src=\"report/1_throughput.png\"\n",
    "  alt=\"Image containing five grouped bars demonstrating the speedup of different optimizations against a baseline with no optimizations\"\n",
    "  style=\"width: 50%; height: auto;\">\n",
    "  <figcaption> <b>Figure 4. </b>Throughput against a baseline with no optimizations for five CNN models. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Table 1 presents the operator patterns exploited in each model for fusion to yield the results in the previous section.  The values in this table were obtained by profiling the ONNX models before and after conversion using an open-source ONNX profiling tool [15]. \n",
    "\n",
    "| Model          | % of Operator Fused|Fusion Pattern(s)       | Original Number of MACs | ONNXSat-Optimized Number of MACs | MAC Reduction | Original Number of Params | ONNXSat-Optimized Number of Params | Params Reduction | Original Memory Usage (Bytes) | ONNXSat-Optimized Memory Usage (Bytes) | Memory Usage Reduction|\n",
    "|----------------|----------------------|----------------------|--------------------------|-----------------------------------|---------------|-----------------------|-----------------------|-----------------------|-----------------------|--------------------------------|--------------------------------|\n",
    "|    [Simple Classifier](data/models/simple_classifier.onnx)            | 57.1%  | Conv -> Relu & Gemm -> Relu                  |    671,050                      |              664,858                     |        0.9%       |  62,006    | 62,006   | 0% |  308,032                |           283,264                     | 8% \n",
    "|   [Resnet18](data/models/resnet18.onnx)     |    22.5%   |        Conv -> Relu              |      1,821,452,264                    |        1,819,896,808                           |         0.085%      | 11,684,712    | 11,684,712   | 0% |  69,727,552                |            63,505,728                    | 8.9% \n",
    "|   [ShuffleNet-v2](data/models/shufflenet-v2.onnx)       |  63.64%    |      Conv -> BatchNorm & Conv -> BatchNorm -> Relu                |       207,580,776                   |                     147,586,744              |        28.9%       |  2,294,784     |  2,270,514     |  1%    |   84,371,520             |                  56,768,584              | 32.7% \n",
    "|     [MobileNetV2](data/models/mobilenetv2.onnx)    |  54.69%     |     Conv -> Clip                 |       319,949,192                   |        307,737,608                            |     3.82%  |   3,487,887  |   3,487,887   |   0%      |       117,982,392                |              69,135,776                   | 41.4%\n",
    "|   [MobileNetV4](data/models/mobilenetv4.onnx)   | 50.85%    |      Conv -> Relu                |               189,867,112           |                188,147,304                   |            0.9%  |  3,761,480     |  3,761,480  |  0%  |     30,297,536           |              23,418,304                   | 22.7%\n",
    "\n",
    "\n",
    "#### 1.2.2. Discussion\n",
    "\n",
    "The results in the previous section demonstrate the benefits of operator fusion, causing a reduction in inference latency, number of multiply-accumulate (MAC) operations and params, and overall memory usage. We also see that some operator fusions are more effective than others. For example, the Conv -> BatchNorm fusion in ShuffleNet-V2 led to the highest MAC reduction percentage of 28.9%. The Conv -> Clip fusion also proves effective, reducing memory usage by ~41% compared to the original model. The Conv -> Relu fusion, however, only leads to marginal reductions in MACs, but still 8-22% reduction in memory usage.\n",
    "\n",
    "##### 1.2.2.1. Latency and Throughput Observations\n",
    "\n",
    "One notable observation from the inference latency plot is that the speedup across models does not directly correlate with reduction in the number of MACs or memory usage. For example, ShuffleNet-V2 achieved a 28.9% reduction in MACs but did not experience the highest latency gain. Instead, ResNet18, which achieved only a modest MAC reduction of 0.085%, exhibited the highest latency speedup of 1.24x.\n",
    "\n",
    "This discrepancy highlights both the importance and challenge of profiling and benchmarking performance metrics. While profiling data such as the percentage of operators fused and the reduction in the number of MACs might suggest that ShuffleNetV2 would lead to the greatest latency improvement, real-world results do not align with this expectation. Other factors such as hardware-specific optimizations and cache effects can affect latency, and may have impacted these measurements despite my best efforts to reduce their effect. \n",
    "\n",
    "We also see throughput gains for the state-of-the-art models that are not directly proportional to their MAC reduction. ONNXSat matches the throughput for the Simple Classifier and we see a reduction for the other optimization methods. However, like with latency, the throughput measurements can be affected by external factors, and the reduction is not significant enough to draw any conclusions from. \n",
    "\n",
    "##### 1.2.2.2. Memory Usage and Parameters\n",
    "The results show that operator fusion also reduces memory usage, with up to 41.4% reduction for MobileNetV2, which is computed for a layer as: `Layer memory usage = Weight tensor memory + Output tensor memory`. We see a reduction in memory usage without reducing the number of params, just by fusing operators. The Conv -> BatchNorm fusion reduces the number of params since the BatchNorm operator gets extra parameters, but we can deduce from the other result that the fusion contributes more to the memory usage reduction. [tk - review]\n",
    "\n",
    "### 1.3. Conclusion\n",
    "\n",
    "Overall, this experiment has provided a quantitative analysis of the effect of operator fusion on ONNX models. By reducing computational requirements and memory usage, operator fusion supports more efficient deployment of models on resource-constrained hardware without reducing the model accuracy or requiring retraining. We also see that the effectiveness of operator fusion depends on the model architecture and the specific operators fused. Future work could explore fusing other patterns like Concat->Reshape, Shape->Gather, and Conv->Add.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How Sparse Can We Go? \n",
    "\n",
    "My second experiment explores pruning weights for optimizing deep neural networks. Keeping with the theme of this project, it aims to answer the question:\n",
    "\n",
    ">  How much sparsity can we introduce into a network without needing to retrain it?\n",
    "\n",
    "Introducing sparsity into a pre-trained model inevitably affects its accuracy. However, this experiment sets a threshold for tolerable accuracy loss and determines how sparse we can make our CNN models while keeping Top-5 accuracy loss within the tolerable threshold.    \n",
    "\n",
    "For this experiment, I referred to the work by Ashouri et al. [16], which introduces three retraining-free methods for pruning convolutional neural networks. The methods differ in how they determine the thresholds they use to set weights with absolute values below the threshold to zero.\n",
    "\n",
    "In the *Flat* method, the same threshold is applied across all the layers. This threshold is determined by profiling the layers in the network to determine the one with the smallest range of weights. This range, referred to as the *span*, serves as an upper bound for the threshold. The final threshold is calculated as a fraction of the span, which can be adjusted as needed.\n",
    "\n",
    "\n",
    "The *Triangular* method is based on the fact that the early convolution layers in many state-of-the-art CNNs have fewer parameters than the later layers. Therefore, it selects thresholds such that the sparsity increases as we go from early layers to later ones. This mitigates accuracy loss from pruning too many neurons in earlier layers. The sparsification threshold differs per layer and falls within a limit based on the span of the first convolution layer and last fully-connected layer.\n",
    "\n",
    "Lastly, the *Relative* method selects a threshold per layer based on the distribution of weights in the layer, where the threshold $T$ of a layer $L$ is the $i^{th}$ percentile of weights in the layer. Weights with absolute values less than or equal to $T$ are set to zero. This addresses a limitation of the previous methods, which may perform poorly for CNNs whose layers exhibit a more complex structure than a linear expansion. For example, some networks replace bigger layers with groups of small ones, which may lead to smaller layers being pruned too aggressively in the flat and triangular methods. \n",
    "\n",
    "\n",
    "\n",
    "This experiment considers the Relative method, implemented in the `_sparsify()` method of the `ModelOptimizer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sparsify(self, model: ModelProto, sparsity_fraction: float) -> ModelProto:\n",
    "    initializer_by_name = {init.name: init for init in model.graph.initializer}\n",
    "\n",
    "    for node in model.graph.node:\n",
    "        if node.op_type == \"Conv\" or node.op_type == \"FusedConv\":\n",
    "            # Get the weight tensor\n",
    "            weights_init = initializer_by_name.get(node.input[1])\n",
    "            weights_tensor = onnx.numpy_helper.to_array(weights_init)\n",
    "            orig_shape = weights_tensor.shape\n",
    "            weights_tensor = weights_tensor.flatten()\n",
    "            \n",
    "            # Determine the dynamic threshold based on the sparsity fraction\n",
    "            threshold = np.percentile(\n",
    "                np.abs(weights_tensor), sparsity_fraction * 100\n",
    "            )\n",
    "            \n",
    "            weights_tensor[np.abs(weights_tensor) < threshold] = 0\n",
    "            weights_tensor = weights_tensor.reshape(orig_shape)\n",
    "            # Replace the weight tensor with the sparsified value\n",
    "            weights_init.CopyFrom(\n",
    "                onnx.numpy_helper.from_array(weights_tensor, weights_init.name)\n",
    "            )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given a sparsity fraction which represents a percentile, `_sparsify()` sets all convolution weights that are less than or equal to the percentile value to zero.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Evaluation\n",
    "\n",
    "This section examines the impact of introducing sparsity through the relative method on model accuracy. The evaluation uses percentile values ranging from 10% to 85%, with steps of 5%, each tested on 5000 samples. I set 5% as the tolerable accuracy loss threshold for this experiment.\n",
    "\n",
    "#### 2.1.1. Experimental Setup\n",
    "The `BenchMarkTool` class in [bench.py](bench/bench.py) contains the method for running this experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_two(self) -> None:\n",
    "    \n",
    "    sparsity_ratios = [ratio.item() for ratio in np.arange(0.1, 0.9, 0.05)]\n",
    "\n",
    "    top_one_accuracy_loss_per_ratio: Dict[str, Dict[float, float]] = defaultdict(\n",
    "        dict\n",
    "    )\n",
    "    top_five_accuracy_loss_per_ratio: Dict[str, Dict[float, float]] = defaultdict(\n",
    "        dict\n",
    "    )\n",
    "\n",
    "     # ... Omitted results storage logic \n",
    "\n",
    "    for model_name, orig_model in self._model_name_to_file.items():\n",
    "\n",
    "        loader, num_labels = self._get_loader_and_num_labels(\n",
    "            model_name, num_samples=5000\n",
    "        )\n",
    "\n",
    "        # Determine baseline \n",
    "        session = ort.InferenceSession(orig_model)\n",
    "        baseline_accuracy = self._measure(session, loader, num_labels).accuracy\n",
    "        base_top_one = baseline_accuracy.top_one\n",
    "        base_top_five = baseline_accuracy.top_five\n",
    "        \n",
    "        \n",
    "        for ratio in sparsity_ratios:\n",
    "            new_model = None\n",
    "            with open(orig_model, \"rb\"):\n",
    "                model_optimizer = ModelOptimizer(\n",
    "                    orig_model, self._data_dir, results_dir\n",
    "                )\n",
    "                new_model = model_optimizer.run(\n",
    "                    sparsity_ratio=ratio, prune_only=True\n",
    "                )\n",
    "\n",
    "            session = ort.InferenceSession(new_model)\n",
    "            accuracy = self._measure(session, loader, num_labels).accuracy\n",
    "            \n",
    "            top_one_accuracy_loss_per_ratio[model_name][ratio] = (\n",
    "                np.round(accuracy.top_one - base_top_one, 2) * 100\n",
    "            ).item()\n",
    "            top_five_accuracy_loss_per_ratio[model_name][ratio] = (\n",
    "                np.round(accuracy.top_five - base_top_five, 2) * 100\n",
    "            ).item()\n",
    "\n",
    "            # Optimization: Stop processing if Top-5 accuracy drops to zero, as it won't get better.\n",
    "            if round(accuracy.top_five, 2) == 0:\n",
    "                break\n",
    "    # ...\n",
    "\n",
    "    self._visualize_accuracy_drop(\"Relative\", 5, top_five_accuracy_loss_per_ratio)\n",
    "    self._visualize_accuracy_drop(\"Relative\", 1, top_one_accuracy_loss_per_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `experiment_2()` method uses the `_measure()` method presented earlier to run inference, and uses a helper `_visualize_accuracy_drop()` method, which receives a dictionary of the computed values, to plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _visualize_accuracy_drop(\n",
    "    self, method: str, top: int, results: Dict[str, Dict[float, float]]\n",
    ") -> None:\n",
    "    # Generate dynamic colors and markers\n",
    "    num_models = len(results.keys())\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, num_models))\n",
    "    markers = [\"o\", \"s\", \"^\", \"D\", \"P\", \"X\", \"*\"]\n",
    "\n",
    "    for i, (model, result_vals) in enumerate(results.items()):\n",
    "        plt.plot(\n",
    "            [0] + list(result_vals.keys()),\n",
    "            [0] + list(result_vals.values()),\n",
    "            linestyle=\"--\",\n",
    "            color=colors[i % len(colors)],\n",
    "            marker=markers[i % len(markers)],\n",
    "            label=model,\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Model Sparsity (%)\")\n",
    "    plt.ylabel(f\"Top-{top} Accuracy Drop (%)\")\n",
    "    plt.title(f\"Top-{top} Accuracy Drop vs Model Sparsity ({method} Method)\")\n",
    "    \n",
    "    # Draw a red line across the -5% threshold on the y axis\n",
    "    plt.axhline(-5, color=\"red\", linewidth=1, linestyle=\"--\")\n",
    "    \n",
    "    plt.xticks(range(0, 90, 10))\n",
    "    plt.xlim(left=0)\n",
    "\n",
    "    plt.legend(loc=\"upper right\", fontsize=10)\n",
    "    plt.grid(True, linestyle=\"-\", alpha=0.5)\n",
    "\n",
    "    plt.minorticks_on()\n",
    "    plt.grid(which=\"minor\", linestyle=\"-\", linewidth=0.5, alpha=0.2)\n",
    "\n",
    "    plt.tick_params(which=\"minor\", length=0)\n",
    "    plt.tick_params(axis=\"both\", which=\"major\", direction=\"in\")\n",
    "    \n",
    "    plt.savefig(f\"{self._results_dir}/2/{method.lower()}_{top}_accuracy_loss.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Results\n",
    "\n",
    "\n",
    "The Top-5 accuracy drop at increasing sparsity levels for the different models is shown in Figure 5 below:\n",
    "\n",
    "<figure style=\"text-align: center; margin: 5px\">\n",
    "  <img\n",
    "  src=\"report/relative_5_accuracy_loss.png\"\n",
    "  alt=\"Top-5 accuracy drop for different models using the relative method at different sparsity levels.\"\n",
    "  style=\"width: 50%; height: auto;\">\n",
    "  <figcaption> <b>Figure 5.</b> Top-5 accuracy drop at different sparsity levels for the five models. The red dotted line indicates the 5% threshold.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Figure 6 presents the Top-1 accuracy results below:\n",
    "\n",
    "<figure style=\"text-align: center; margin: 5px\">\n",
    "  <img\n",
    "  src=\"report/relative_1_accuracy_loss.png\"\n",
    "  alt=\"Top-1 accuracy drop for different models using the relative method at different sparsity levels.\"\n",
    "  style=\"width: 50%; height: auto;\">\n",
    "  <figcaption> <b>Figure 6.</b> Top-1 accuracy drop at different sparsity levels for the five models. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "\n",
    "#### 2.1.3 Discussion\n",
    "\n",
    "[tk - add table for model accuracy and stuff]\n",
    "\n",
    "The results validate the effect of the architecture of the model on the accuracy loss caused by introducing sparsity. Figure 5 shows that we can gain up to 30% sparsity (1.43x compression factor) for ShuffleNet-V2 and ResNet18 while keeping Top-5 accuracy loss within 5%. For the simple classifier, we can gain up to 70% sparsity (3.33x compression factor) while keeping accuracy loss within the threshold. However, MobileNetV2 and MobileNetV4 do not play well with the relative method for sparsification, dropping below 5% accuracy loss at 14% and 1% sparsity respectively.\n",
    "\n",
    "This is because of the structure of the models. In the MobileNet architectures, the size of the convolution layers increases linearly, and the relative method over-prunes the earlier layers. This is in contrast to ShuffleNet-V2, for example, where the size is more consistent across layers.\n",
    "\n",
    "\n",
    "Figure 6 shows a similar trend for the Top-1 accuracy, which remains within the 5% threshold as we gain up to 26% and 30% sparsity for ShuffleNetV2 and ResNet18 respectively. The simple classifier can gain 50% sparsity (2x compression) and still be within 5% of its original accuracy.\n",
    "\n",
    "\n",
    "### 2.2. Conclusion\n",
    "This experiment has evaluated the relative method for sparsification without retraining. Sparsification without retraining provides the benefits of sparsification when there is no access to training data or retraining is expensive. Using the relative method, we can get up to 30% sparsity with less than 5% drop in Top-5 inference accuracy. However, the performance of a sparsification method depends on the model architecture, and we have seen how the relative method is not effective for the MobileNet architecture. Future work can explore other implementing other sparsification methods and training a classifier to predict what method to use based on the model architecture, as done by Ashouri et al [16]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What Does Sparsity Give Us?\n",
    "\n",
    "Sparsifying a model's weights is more effective when the presence of zero values can be exploited on hardware. In this experiment, I examine the impact of sparsifying a model on its performance. Ashouri et al [16] claim in their paper that sparsifying convolution weights instead of input data, for example, \"alleviates the need for specialized hardware accelerators to exploit the sparsity.\" \n",
    "\n",
    "This experiment measures the performance of models after sparsifying their weights. First, I adapt code from the official ONNX runtime codebase [17] to convert sparsified weights in a model to ONNX `SparseTensor` representations. This code is located in the [sparsify_initializers.py](sparsify_initializers.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "# --------------------------------------------------------------------------\n",
    "# This script opens an existing model in onnx format and attempts to\n",
    "# move initializers from model.graph.initializer field to model.graph.sparse_initializer field\n",
    "# and convert them into ONNX COO flat index format.\n",
    "\n",
    "# Adapted code from : https://github.com/microsoft/onnxruntime/blob/main/tools/python/sparsify_initializers.py\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import ModelProto, SparseTensorProto, TensorProto, numpy_helper\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "log_handler = logging.StreamHandler(sys.stdout)\n",
    "log_handler.setFormatter(logging.Formatter(\"%(filename)20s: %(message)s\"))\n",
    "logging_level = logging.ERROR\n",
    "log_handler.setLevel(logging_level)\n",
    "logger.addHandler(log_handler)\n",
    "logger.setLevel(logging_level)\n",
    "\n",
    "real_types = {int(TensorProto.FLOAT), int(TensorProto.DOUBLE)}\n",
    "\n",
    "def convert_tensor_to_sparse(tensor, sparsity_threshold, tolerance):  # type: (TensorProto, float, float) -> Tuple[SparseTensorProto, float]\n",
    "    \"\"\"returns a tuple of sparse_tensor and sparsity level\"\"\"\n",
    "    values = []\n",
    "    indices = []\n",
    "    nnz_count = 0\n",
    "    tensor_data = numpy_helper.to_array(tensor).flatten()\n",
    "    data_len = len(tensor_data)\n",
    "\n",
    "    if tensor_data.dtype in real_types:\n",
    "        for index in range(data_len):\n",
    "            el = tensor_data[index]\n",
    "            if abs(el) <= tolerance: \n",
    "                values.append(el)\n",
    "                indices.append(index)\n",
    "                nnz_count += 1\n",
    "    else:\n",
    "        for index in range(data_len):\n",
    "            el = tensor_data[index]\n",
    "            if el != 0:\n",
    "                values.append(el)\n",
    "                indices.append(index)\n",
    "                nnz_count += 1\n",
    "\n",
    "    sparsity = 1.0 - float(nnz_count) / data_len\n",
    "\n",
    "    ind_data_type = TensorProto.INT64\n",
    "    ind_dtype = np.int64\n",
    "    ind_len = len(indices)\n",
    "\n",
    "    sparsity = np.round(sparsity, 2)\n",
    "    if sparsity < sparsity_threshold:\n",
    "        return (object(), sparsity)\n",
    "\n",
    "    # create np array and cast data to the appropriate type\n",
    "    np_values = np.array(values).astype(tensor_data.dtype)\n",
    "    # create np array and cast data to the inferred index type\n",
    "    np_indices = np.array(indices).astype(ind_dtype)\n",
    "\n",
    "    values_tensor = onnx.helper.make_tensor(\n",
    "        tensor.name, tensor.data_type, [len(values)], np_values.tobytes(), raw=True\n",
    "    )\n",
    "\n",
    "    indicies_tensor = onnx.helper.make_tensor(\n",
    "        tensor.name + \"_indicies\",\n",
    "        ind_data_type,\n",
    "        [ind_len],\n",
    "        np_indices.tobytes(),\n",
    "        raw=True,\n",
    "    )\n",
    "\n",
    "    sparse_tensor = onnx.helper.make_sparse_tensor(\n",
    "        values_tensor, indicies_tensor, tensor.dims\n",
    "    )\n",
    "    return (sparse_tensor, sparsity)\n",
    "\n",
    "\n",
    "def convert_initializers(model, sparsity_threshold, tolerance):  # type: (ModelProto, float, float) -> ModelProto\n",
    "    graph = model.graph\n",
    "    converted_sparse = []\n",
    "    remaining_initializers = []\n",
    "    for initializer in graph.initializer:\n",
    "        if initializer.data_type == TensorProto.BOOL:\n",
    "            remaining_initializers.append(initializer)\n",
    "            continue\n",
    "        sparse_tensor, sparsity = convert_tensor_to_sparse(\n",
    "            initializer, sparsity_threshold, tolerance\n",
    "        )\n",
    "\n",
    "        if sparsity >= sparsity_threshold:\n",
    "            converted_sparse.append(sparse_tensor)\n",
    "        else:\n",
    "            remaining_initializers.append(initializer)\n",
    "\n",
    "\n",
    "    graph.sparse_initializer.extend(converted_sparse)\n",
    "    del graph.initializer[:]\n",
    "    graph.initializer.extend(remaining_initializers)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1. Evaluation\n",
    "\n",
    "To evaluate the impact of sparsification, I choose the highest sparsity value within the 5% Top-5 accuracy drop threshold for each model and record the inference latency and throughput.\n",
    "\n",
    "\n",
    "#### 3.1.1. Experimental Setup\n",
    "\n",
    "The method below defined in `BenchmarkTool` performs the experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_three(self) -> None:\n",
    "    results: Dict[str, Dict[BenchMarkCategory, Benchmark]] = {}\n",
    "\n",
    "\n",
    "    # Max sparsity ratio using the results in experiment 2 that keeps the results within the 5% accuracy drop threshold\n",
    "    max_sparsity_ratio_per_model = {\n",
    "        \"simple_classifier\": 0.74,\n",
    "        \"resnet18\": 0.32,\n",
    "        \"mobilenetv2\": 0.14,\n",
    "        \"mobilenetv4\": 0.01,\n",
    "        \"shufflenet-v2\": 0.28,\n",
    "    }\n",
    "\n",
    "    for model_name, orig_model in self._model_name_to_file.items():\n",
    "        sparsity_ratio = max_sparsity_ratio_per_model[model_name]\n",
    "\n",
    "        onnx_optimized_model = (\n",
    "            f\"{self._onnx_optimized_models_dir}/{model_name}.onnx\"\n",
    "        )\n",
    "        self.apply_onnx_opt_offline(model_name, orig_model, onnx_optimized_model)\n",
    "        \n",
    "        new_model = None\n",
    "        with open(orig_model, \"rb\"):\n",
    "            model_optimizer = ModelOptimizer(\n",
    "                orig_model, self._data_dir, results_dir\n",
    "            )\n",
    "            new_model = model_optimizer.run(sparsity_ratio=sparsity_ratio)\n",
    "        \n",
    "        category_to_model = {\n",
    "            BenchMarkCategory.ONNX_SAT: new_model,\n",
    "            BenchMarkCategory.ONNX_OPT: onnx_optimized_model,\n",
    "            BenchMarkCategory.BASELINE: orig_model,\n",
    "        }\n",
    "        category_results = {}\n",
    "        \n",
    "        warmup_loader, num_labels = self._get_loader_and_num_labels(\n",
    "            model_name, num_samples=1000\n",
    "        )\n",
    "        loader, num_labels = self._get_loader_and_num_labels(\n",
    "            model_name, num_samples=2000\n",
    "        )\n",
    "        \n",
    "        for category, model in category_to_model.items():\n",
    "            sess_options = ort.SessionOptions()\n",
    "            sess_options.graph_optimization_level = (\n",
    "                ort.GraphOptimizationLevel.ORT_DISABLE_ALL\n",
    "            )\n",
    "            session = ort.InferenceSession(model, sess_options=sess_options)\n",
    "            \n",
    "            # Warm up\n",
    "            self._measure(session, warmup_loader, num_labels)\n",
    "            \n",
    "            # Actual Measurment\n",
    "            category_results[category] = self._measure(\n",
    "                session, loader, num_labels, num_runs=3\n",
    "            )\n",
    "        \n",
    "        # Format the map keys, as they form the x-axis labels of the plot\n",
    "        results[f\"{model_name}\\n ({round(sparsity_ratio*100)}% sparsity)\"] = (\n",
    "            category_results\n",
    "        )\n",
    "\n",
    "    self._visualize_latency_and_throughput(3, category_to_model.keys(), results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1.2. Results\n",
    "Figure 7 shows the speedup gained by combining sparsification and operator fusion. Only the blue bar representing ONNXSat runs the pruned model. The baseline and ONNX optimization measurements are taken from running the unpruned model. \n",
    "\n",
    "\n",
    "<figure style=\"text-align: center; margin: 5px\">\n",
    "  <img\n",
    "  src=\"report/3_latency.png\"\n",
    "  alt=\"Top-1 accuracy drop for different models using the relative method at different sparsity levels.\"\n",
    "  style=\"width: 50%; height: auto;\">\n",
    "  <figcaption> <b>Figure 7. </b> Median latency speedup after sparsification against a baseline with no optimizations for five CNN models. </figcaption>\n",
    "</figure>\n",
    "\n",
    "Figure X shows the throughput values:\n",
    "\n",
    "<figure style=\"text-align: center; margin: 5px\">\n",
    "  <img\n",
    "  src=\"report/3_throughput.png\"\n",
    "  alt=\"Top-1 accuracy drop for different models using the relative method at different sparsity levels.\"\n",
    "  style=\"width: 50%; height: auto;\">\n",
    "  <figcaption> <b>Figure 8. </b> Throughput against a baseline with no optimizations for five CNN models. </figcaption>\n",
    "</figure>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3. Discussion\n",
    "\n",
    "The results show that for the state-of-the-art models, we can gain performance benefits from sparsifying a model even without specialized hardware accelerators exploiting sparsity. We introduce the highest sparsity in the ResNet18 model (32%) and that gives us the highest performance gain of 1.4x, and 1.35x over the equivalent dense model with all ONNX runtime optimizations applied.   \n",
    "\n",
    "Similarly, introducing sparsity increases the throughput of each model, with models with higher sparsity getting better throughput. The exception to this trend is the simple classifier, which achieves slightly worse latency and throughput than the unpruned version. However, as it is a small model. It is difficult to draw conclusions from it. [tk - rewrite]\n",
    "\n",
    "### 3.2. Conclusion\n",
    "\n",
    "Unfortunately, technical constraints due to the poor support for sparse tensors in the ONNX profiler and time constraints prevent me from conducting an analysis of the number of MACs and memory reduction gained by sparsifying the tensors, as done in Experiment 1. That is left as future work.  \n",
    "\n",
    "Overall, the results show that without losing more than 5% of Top-5 accuracy, we can gain improve the latency of our model by up to 40%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Limitations and Future Work\n",
    "The results show what we can gain by without retraining a model by implementing operator fusion and sparsification. However, there are a few directions this could go in for most robust experimentation:\n",
    "\n",
    "- **Implementing more rewrite rules:** Despite the rewrite rules in ONNXSat exploiting most of the patterns in the evaluated models. However, there are more operator fusion rules that can be implemented for ONNX models, such as fusing Shape -> Gather sequences into one operation and doing the same for Concat -> Reshape.\n",
    "- **Evaluating on larger models:** Egglog's extraction is slow and I ran out of memory for some larger models like ResNet-50, DenseNet, EfficientNet, InceptionV3, and newer vison transformer models. There is also room to experiment on \n",
    "- **Developing a better cost model for equality saturation**: Empirical measurement.\n",
    "- **Implementing different sparsification methods**: Finally, there is room to implement the Flat and Triangle sparsification methods presented in the paper by Ashouri et al. [tk - cite], as well as develop a classifier to determine what sparsification method to use.  \n",
    "\n",
    "\n",
    "\n",
    "## 5. Conclusion\n",
    "This project aimed to explore what we can gain in the performance of neural network models without retraining a model....\n",
    "\n",
    "## References\n",
    "\n",
    "[1] R. Tate, M. Stepp, Z. Tatlock, and S. Lerner, ‘Equality saturation: a new approach to optimization’, in Proceedings of the 36th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages, in POPL ’09. New York, NY, USA: Association for Computing Machinery, Jan. 2009, pp. 264–276. doi: 10.1145/1480881.1480915.\n",
    "\n",
    "[2] M. Willsey, C. Nandi, Y. R. Wang, O. Flatt, Z. Tatlock, and P. Panchekha, ‘egg: Fast and extensible equality saturation’, Proc. ACM Program. Lang., vol. 5, no. POPL, pp. 1–29, Jan. 2021, doi: 10.1145/3434304.\n",
    "\n",
    "[3] Y. Yang, P. M. Phothilimtha, Y. R. Wang, M. Willsey, S. Roy, and J. Pienaar, ‘Equality Saturation for Tensor Graph Superoptimization’, Mar. 17, 2021, arXiv: arXiv:2101.01332. Accessed: Nov. 12, 2024. [Online]. Available: http://arxiv.org/abs/2101.01332\n",
    "\n",
    "[4] J. Hartmann, G. He, and E. Yoneki, ‘Optimizing Tensor Computation Graphs with Equality Saturation and Monte Carlo Tree Search’, in Proceedings of the 2024 International Conference on Parallel Architectures and Compilation Techniques, Long Beach CA USA: ACM, Oct. 2024, pp. 40–52. doi: 10.1145/3656019.3689611.\n",
    "\n",
    "[5] Z. Jia, O. Padon, J. Thomas, T. Warszawski, M. Zaharia, and A. Aiken, ‘TASO: optimizing deep learning computation with automatic generation of graph substitutions’, in Proceedings of the 27th ACM Symposium on Operating Systems Principles, Huntsville Ontario Canada: ACM, Oct. 2019, pp. 47–62. doi: 10.1145/3341301.3359630.\n",
    "\n",
    "[6] S. Shanabrook, Egglog Python: A Pythonic Library for E-graphs. [Online]. Available: https://github.com/egraphs-good/egglog-python\n",
    "\n",
    "[7] ‘onnxruntime/docs/ContribOperators.md at main · microsoft/onnxruntime’, GitHub. Available: https://github.com/microsoft/onnxruntime/blob/main/docs/ContribOperators.md\n",
    "\n",
    "[8] Nathan Hubens, ‘Faster Inference - Batch Normalization Folding’, fast.ai Course Forums. [Online]. Available: https://forums.fast.ai/t/faster-inference-batch-normalization-folding/69161\n",
    "\n",
    "[9] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, ‘MobileNetV2: Inverted Residuals and Linear Bottlenecks’, Mar. 21, 2019, arXiv: arXiv:1801.04381. doi: 10.48550/arXiv.1801.04381.\n",
    "\n",
    "[10] D. Qin et al., ‘MobileNetV4 -- Universal Models for the Mobile Ecosystem’, Sep. 29, 2024, arXiv: arXiv:2404.10518. doi: 10.48550/arXiv.2404.10518.\n",
    "\n",
    "[11] K. He, X. Zhang, S. Ren, and J. Sun, ‘Deep Residual Learning for Image Recognition’, in 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2016, pp. 770–778. doi: 10.1109/CVPR.2016.90.\n",
    "\n",
    "[12] N. Ma, X. Zhang, H.-T. Zheng, and J. Sun, ‘ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design’, Jul. 30, 2018, arXiv: arXiv:1807.11164. doi: 10.48550/arXiv.1807.11164.\n",
    "\n",
    "[13] O. Russakovsky et al., ‘ImageNet Large Scale Visual Recognition Challenge’, Jan. 30, 2015, arXiv: arXiv:1409.0575. doi: 10.48550/arXiv.1409.0575.\n",
    "\n",
    "[14] A. Krizhevsky, ‘Learning Multiple Layers of Features from Tiny Images’, 2009. [Online]. Available: https://api.semanticscholar.org/CorpusID:18268744\n",
    "\n",
    "[15] ‘ThanatosShinji/onnx-tool: A parser, editor and profiler tool for ONNX models.’ Accessed: Jan. 21, 2025. [Online]. Available: https://github.com/ThanatosShinji/onnx-tool\n",
    "\n",
    "[16] A. H. Ashouri, T. S. Abdelrahman, and A. Dos Remedios, ‘Retraining-free methods for fast on-the-fly pruning of convolutional neural networks’, Neurocomputing, vol. 370, pp. 56–69, Dec. 2019, doi: 10.1016/j.neucom.2019.08.063.\n",
    "\n",
    "[17] ONNX Runtime developers, ONNX Runtime. (Nov. 2018). [Online]. Available: https://github.com/microsoft/onnxruntime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
